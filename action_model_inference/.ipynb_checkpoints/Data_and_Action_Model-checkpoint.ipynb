{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L4Yi4cFgJtbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Warning: No domain path is set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import string\n",
    "import os\n",
    "\n",
    "from macq import generate, extract\n",
    "from macq.observation import IdentityObservation, AtomicPartialObservation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8ze3QG7GJ8X"
   },
   "source": [
    "# First-order Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Type:\n",
    "    def __init__(self, name, parent):\n",
    "        self.name = name\n",
    "        self.parent = parent\n",
    "        \n",
    "    def is_child(self, another_type):\n",
    "        if self.name == another_type.name:\n",
    "            return True\n",
    "        elif self.parent is None:\n",
    "            return False\n",
    "        else:\n",
    "            return self.parent.is_child(another_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DdgyvHYyJMa-"
   },
   "outputs": [],
   "source": [
    "class Predicate:\n",
    "  def __init__(self, name, params):\n",
    "    self.name = name\n",
    "    # params are dicts {Type: num}\n",
    "    self.params = params\n",
    "    self.params_types = sorted(params.keys(), key=lambda x: x.name)\n",
    "\n",
    "  def proposition(self, sorted_obj_lists):\n",
    "    return (self.name + ' ' + ' '.join([f'{self.params_types[i].name} '+f' {self.params_types[i].name} '.join(sorted_obj_lists[i])\n",
    "                                       for i in range(len(sorted_obj_lists))])).strip()\n",
    "\n",
    "  def ground(self, objects):\n",
    "    '''\n",
    "    Input a list of objects in the form {Type: []}\n",
    "    Return all the propositions grounded from this predicates with the objects\n",
    "    '''\n",
    "    propositions = []\n",
    "    obj_lists_per_params = {params_type:[] for params_type in self.params_types}\n",
    "    for params_type in self.params_types:\n",
    "        for obj_type in objects.keys():\n",
    "            if obj_type.is_child(params_type):\n",
    "                obj_lists_per_params[params_type].extend(objects[obj_type])\n",
    "    for obj_lists in itertools.product(*[itertools.permutations(obj_lists_per_params[params_type], self.params[params_type])\\\n",
    "                                        for params_type in self.params_types]):\n",
    "      propositions.append(self.proposition(obj_lists))\n",
    "    return propositions\n",
    "    \n",
    "  def ground_num(self, objects):\n",
    "    '''\n",
    "    Return how many propositions this predicate can ground on the objects\n",
    "    '''\n",
    "    n_ground = 1\n",
    "    for params_type in self.params_types:\n",
    "      n_obj = 0\n",
    "      for obj_type in objects.keys():\n",
    "        if obj_type.is_child(params_type):\n",
    "          n_obj += len(objects[obj_type])\n",
    "      n_ground *= math.perm(n_obj, self.params[params_type])\n",
    "    return n_ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Zwu9BAAtPXBj"
   },
   "outputs": [],
   "source": [
    "class Action_Schema(nn.Module):\n",
    "  def __init__(self, name, params):\n",
    "    super(Action_Schema, self).__init__()\n",
    "    self.name = name\n",
    "    # params are dicts {Type: num}\n",
    "    self.params = params\n",
    "    self.params_types = sorted(params.keys(), key=lambda x: x.name)\n",
    "    # predicates that are relevant\n",
    "    self.predicates = []\n",
    "\n",
    "  def initialise(self, predicates, device):\n",
    "    '''\n",
    "    Input all predicates and generate the model for action schema\n",
    "    '''\n",
    "    n_features = 0\n",
    "    for predicate in predicates:\n",
    "      # A predicate is relevant to an action schema iff for each of its param type,\n",
    "      # the number of objects required is leq the number of objects there is \n",
    "      # for the same type or children type in the action schema\n",
    "      is_relevant = True\n",
    "      # Also calculate how many propositions there are when predicate is grounded on \"variables\"\n",
    "      # e.g. on X Y; on Y X when X and Y are variables\n",
    "      n_ground = 1\n",
    "      for params_type in predicate.params_types:\n",
    "        n_params = 0\n",
    "        for model_params_type in self.params:\n",
    "          if model_params_type.is_child(params_type):\n",
    "            n_params += self.params[model_params_type]\n",
    "        if predicate.params[params_type]>n_params:\n",
    "          is_relevant = False\n",
    "          break\n",
    "        else:\n",
    "          n_ground *= math.perm(n_params, predicate.params[params_type])\n",
    "      if is_relevant:\n",
    "        self.predicates.append(predicate)\n",
    "        n_features += n_ground\n",
    "    n_features = int(n_features)\n",
    "    \n",
    "    self.randn = torch.randn(n_features, 128, device=device, requires_grad=True)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 4),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "    self.mlp.to(device)\n",
    "    \n",
    "  def forward(self):\n",
    "    return self.mlp(self.randn)\n",
    "\n",
    "  def ground(self, objects, is_single_action=False):\n",
    "    if is_single_action:\n",
    "      propositions = []\n",
    "      for predicate in self.predicates:\n",
    "        propositions.extend(predicate.ground(objects))\n",
    "      return propositions\n",
    "    else:\n",
    "      propositions = []\n",
    "      obj_lists_per_params = {params_type:[] for params_type in self.params_types}\n",
    "      for params_type in self.params_types:\n",
    "        for obj_type in objects.keys():\n",
    "          if obj_type.is_child(params_type):\n",
    "            obj_lists_per_params[params_type].extend(objects[obj_type])  \n",
    "      for obj_list in itertools.product(*[itertools.permutations(obj_lists_per_params[params_type], self.params[params_type])\\\n",
    "                                          for params_type in self.params_types]):\n",
    "        objects_per_action = {}\n",
    "        for i in range(len(self.params_types)):\n",
    "          objects_per_action[self.params_types[i]] = obj_list[i]\n",
    "        propositions_per_action = []\n",
    "        for predicate in self.predicates:\n",
    "          propositions_per_action.extend(predicate.ground(objects_per_action))\n",
    "        propositions.append(propositions_per_action)\n",
    "      return propositions\n",
    "    \n",
    "  def pretty_print(self):\n",
    "    var = {}\n",
    "    n = 0\n",
    "    for param_type in self.params_types:\n",
    "      var[param_type] = list(string.ascii_lowercase)[n:n+self.params[param_type]]\n",
    "      n += self.params[param_type]\n",
    "    print(f'{self.name}' + ' ' + ' '.join([k.name+' '+v for k in var.keys() for v in var[k]]))\n",
    "    propositions = self.ground(var, True)\n",
    "    precon_list = []\n",
    "    addeff_list = []\n",
    "    deleff_list = []\n",
    "    result = torch.argmax(self(), dim=1)\n",
    "    for i in range(len(propositions)):\n",
    "        if result[i]==1:\n",
    "            addeff_list.append(propositions[i])\n",
    "        elif result[i]==2:\n",
    "            precon_list.append(propositions[i])\n",
    "        elif result[i]==3:\n",
    "            precon_list.append(propositions[i])\n",
    "            deleff_list.append(propositions[i])\n",
    "    print(', '.join(precon_list))\n",
    "    print(', '.join(addeff_list))\n",
    "    print(', '.join(deleff_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CFznpcIAPg0l"
   },
   "outputs": [],
   "source": [
    "class Domain_Model(nn.Module):\n",
    "  def __init__(self, predicates, action_schemas, device):\n",
    "    super(Domain_Model, self).__init__()\n",
    "    self.predicates = predicates\n",
    "    self.action_schemas = action_schemas\n",
    "    self.device = device\n",
    "    for action_schema in action_schemas:\n",
    "      action_schema.initialise(predicates, self.device)\n",
    "\n",
    "  def ground(self, objects):\n",
    "    # Ground predicates to propositions\n",
    "    # Record in a dictionary with values as indices, for later lookup\n",
    "    self.propositions = {}\n",
    "    for predicate in self.predicates:\n",
    "      for proposition in predicate.ground(objects):\n",
    "        self.propositions[proposition] = len(self.propositions)\n",
    "\n",
    "    # For each action schema, ground to actions and then find the indices\n",
    "    self.indices = []\n",
    "    # Also need to know which action schema each action is from\n",
    "    self.action_to_schema = []\n",
    "    for action_schema in self.action_schemas:\n",
    "      for propositions in action_schema.ground(objects):\n",
    "        self.indices.append([self.propositions[p] for p in propositions])\n",
    "        self.action_to_schema.append(action_schema)\n",
    "        \n",
    "\n",
    "  def build(self, actions):\n",
    "    '''\n",
    "    actions is a list of numbers\n",
    "    '''\n",
    "    precon = torch.zeros((len(actions), len(self.propositions)), device=self.device, requires_grad=False)\n",
    "    addeff = torch.zeros((len(actions), len(self.propositions)), device=self.device, requires_grad=False)\n",
    "    deleff = torch.zeros((len(actions), len(self.propositions)), device=self.device, requires_grad=False)\n",
    "    for i in range(len(actions)):\n",
    "      y_indices = self.indices[actions[i]]\n",
    "      schema = self.action_to_schema[actions[i]]\n",
    "      y_indices_set = set(y_indices)\n",
    "      \n",
    "      schema_prams = schema()\n",
    "      schema_precon = schema_prams @ torch.tensor([0.0, 0.0, 1.0, 1.0], device=self.device)\n",
    "      schema_addeff = schema_prams @ torch.tensor([0.0, 1.0, 0.0, 0.0], device=self.device)\n",
    "      schema_deleff = schema_prams @ torch.tensor([0.0, 0.0, 0.0, 1.0], device=self.device)\n",
    "\n",
    "      if len(y_indices)>len(y_indices_set):\n",
    "        # There are duplicate indices in y_indices\n",
    "        # Multiple predicates are grounded to one same proposition\n",
    "        # We need to combine the contribution from different predicates to one proposition\n",
    "        applied = set()\n",
    "        for y_idx in y_indices:\n",
    "          if y_idx not in applied:\n",
    "            precon[i, y_idx] += schema_precon[y_idx]\n",
    "            addeff[i, y_idx] += schema_addeff[y_idx]\n",
    "            deleff[i, y_idx] += schema_deleff[y_idx]\n",
    "            applied.add(y_idx)\n",
    "          else:\n",
    "            # The multiple effects are combined with \"or\"\n",
    "            # p v q = not ((not p)^(not q))\n",
    "            precon[i, y_idx] = 1 - (1-precon[i, y_idx])*(1-schema_precon[y_idx])\n",
    "            addeff[i, y_idx] = 1 - (1-addeff[i, y_idx])*(1-schema_addeff[y_idx])\n",
    "            deleff[i, y_idx] = 1 - (1-deleff[i, y_idx])*(1-schema_deleff[y_idx])\n",
    "      else:\n",
    "        x_indices = [i]*len(y_indices)\n",
    "        precon[x_indices, y_indices] += schema_precon\n",
    "        addeff[x_indices, y_indices] += schema_addeff\n",
    "        deleff[x_indices, y_indices] += schema_deleff\n",
    "    return precon, addeff, deleff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Type(\"object\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Domain_Model(\n",
    "    [\n",
    "        Predicate('arm-empty', {}),\n",
    "        Predicate('clear', {obj:1}),\n",
    "        Predicate('on-table', {obj:1}),\n",
    "        Predicate('holding', {obj:1}),\n",
    "        Predicate('on', {obj:2}),\n",
    "    ],\n",
    "    [\n",
    "        Action_Schema('pickup', {obj:1}),\n",
    "        Action_Schema('putdown', {obj:1}),\n",
    "        Action_Schema('stack', {obj:2}),\n",
    "        Action_Schema('unstack', {obj:2}),\n",
    "    ]\n",
    ", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = {obj: ['block1', 'block2', 'block3', 'block4', 'block5']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ground(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grounded_actions = {}\n",
    "for action_schema in model.action_schemas:\n",
    "    obj_lists_per_params = {params_type:[] for params_type in action_schema.params_types}\n",
    "    for params_type in action_schema.params_types:\n",
    "        for obj_type in objects.keys():\n",
    "            if obj_type.is_child(params_type):\n",
    "                obj_lists_per_params[params_type].extend(objects[obj_type])  \n",
    "    for obj_list in itertools.product(*[itertools.permutations(obj_lists_per_params[params_type], action_schema.params[params_type])\\\n",
    "                                          for params_type in action_schema.params_types]):\n",
    "        objects_per_action = {}\n",
    "        constructed = action_schema.name + ' ' + ' '.join([f'{action_schema.params_types[i].name} '\n",
    "                                                           +f' {action_schema.params_types[i].name} '.join(obj_list[i])\n",
    "                                                           for i in range(len(obj_list))])\n",
    "        all_grounded_actions[constructed] = len(all_grounded_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 110.34it/s]\n"
     ]
    }
   ],
   "source": [
    "steps_state1 = []\n",
    "steps_action = []\n",
    "steps_state2 = []\n",
    "\n",
    "traces = generate.pddl.VanillaSampling(dom='./blockworld/domain.pddl', prob='./blockworld/prob01.pddl', plan_len = 10, num_traces = 10).traces\n",
    "for trace in traces:\n",
    "    # last step no action\n",
    "    for t in range(len(trace.steps)-1):\n",
    "        fluents_in_state1 = {f._serialize()[1:-1] for f in trace.steps[t].state if trace.steps[t].state[f] is True}\n",
    "        fluents_in_state2 = {f._serialize()[1:-1] for f in trace.steps[t+1].state if trace.steps[t+1].state[f] is True}\n",
    "        state1 = [1 if p in fluents_in_state1 else 0 for p in model.propositions]\n",
    "        state2 = [1 if p in fluents_in_state2 else 0 for p in model.propositions]\n",
    "        \n",
    "        # action parameter order may not be the same as our model\n",
    "        action = trace.steps[t].action\n",
    "        action_obj_params = sorted([o for o in action.obj_params], key=lambda o:o.obj_type)\n",
    "        \n",
    "        steps_action.append(all_grounded_actions[f\"{action.name} {' '.join([o.details()for o in action_obj_params])}\"])\n",
    "        \n",
    "        steps_state1.append(state1)\n",
    "        steps_state2.append(state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_state1_tensor = torch.tensor(np.array(steps_state1)).float()\n",
    "steps_action_tensor = torch.tensor(np.array(steps_action))\n",
    "steps_state2_tensor = torch.tensor(np.array(steps_state2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1000\n",
    "dataset = TensorDataset(steps_state1_tensor, steps_action_tensor, steps_state2_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_sz, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "for schema in model.action_schemas:\n",
    "    parameters.append({'params': schema.parameters(), 'lr': 1e-3})\n",
    "optimizer = optim.Adam(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 RESULTS: Average loss: 0.8820755615\n",
      "Epoch 10 RESULTS: Average loss: 0.8452726440\n",
      "Epoch 20 RESULTS: Average loss: 0.7793428955\n",
      "Epoch 30 RESULTS: Average loss: 0.6977715454\n",
      "Epoch 40 RESULTS: Average loss: 0.6401904297\n",
      "Epoch 50 RESULTS: Average loss: 0.6134895630\n",
      "Epoch 60 RESULTS: Average loss: 0.6045328979\n",
      "Epoch 70 RESULTS: Average loss: 0.5974949341\n",
      "Epoch 80 RESULTS: Average loss: 0.5947376709\n",
      "Epoch 90 RESULTS: Average loss: 0.5938486328\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "  optimizer.zero_grad()\n",
    "  loss_final = 0.0\n",
    "  for i, (state_1, executed_actions, state_2) in enumerate(dataloader):\n",
    "    precon, addeff, deleff = model.build(executed_actions)\n",
    "    # The result of applying a in s is (s\\Del(a)) U Add(a)\n",
    "    # We can simplfy this to be:\n",
    "    # ((p in state 1) ^ (not p in Del(a))) v ((not p in state 1) ^ (p in Add(a)))\n",
    "    # Note we implicitly apply the constraint that add effects and preconditions\n",
    "    # cannot intersect and only preconditions can be deleted\n",
    "    # The \"or\" can be translated to an addition as the two sides and exclusive\n",
    "#     preds = addeff + (1-addeff)*state_1*(1-deleff)\n",
    "    preds = 1- (1-state_1*(1-deleff)) * (1-(1-state_1)*addeff)\n",
    "    \n",
    "    # Since we view the state_2 as true targets, we can binary cross-entropy loss\n",
    "    # If state_2 is also predicated, use KL-divergence to ensure two distributions are close?\n",
    "    loss = F.mse_loss(preds, state_2, reduction='sum')\n",
    "    # Add in validity constraint\n",
    "    # Since executed actions are applicable in state_1\n",
    "    # p in Pre(a) -> p in state_1 for all a in executed_actions\n",
    "    # not ((p in Pre(a)) ^ (not p in state_1))\n",
    "    validity_constraint = (1-state_1) * (precon)\n",
    "    loss += F.mse_loss(validity_constraint, torch.zeros(validity_constraint.shape, dtype=validity_constraint.dtype), reduction='sum')\n",
    "    loss += 0.2*F.mse_loss(precon, torch.ones(precon.shape, dtype=precon.dtype), reduction='sum')\n",
    "#     loss += model.constraint_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_final += loss.item() / batch_sz\n",
    "  if epoch%10 == 0:\n",
    "    print('Epoch {} RESULTS: Average loss: {:.10f}'.format(epoch, loss_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup object a\n",
      "arm-empty, clear object a, on-table object a\n",
      "holding object a\n",
      "arm-empty, clear object a, on-table object a\n",
      "\n",
      "putdown object a\n",
      "holding object a\n",
      "arm-empty, clear object a, on-table object a\n",
      "holding object a\n",
      "\n",
      "stack object a object b\n",
      "clear object b, holding object a\n",
      "arm-empty, clear object a, on object a object b\n",
      "clear object b, holding object a\n",
      "\n",
      "unstack object a object b\n",
      "arm-empty, clear object a, on object a object b\n",
      "clear object b, holding object a\n",
      "arm-empty, clear object a, on object a object b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for action_schema in model.action_schemas:\n",
    "    action_schema.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gripper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Problem Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Type(\"object\", None)\n",
    "room = Type(\"room\", base)\n",
    "ball = Type(\"ball\", base)\n",
    "gripper = Type(\"gripper\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Domain_Model([\n",
    "                        Predicate('at-robby', {room:1}),\n",
    "                        Predicate('at', {ball:1, room:1}),\n",
    "                        Predicate('free', {gripper:1}),\n",
    "                        Predicate('carry', {ball:1, gripper:1}),],\n",
    "                     [\n",
    "                        Action_Schema('move', {room:2}),\n",
    "                        Action_Schema('pick', {ball:1, room:1, gripper:1}),\n",
    "                        Action_Schema('drop', {ball:1, room:1, gripper:1}),\n",
    "                     ], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = {\n",
    "    room: ['rooma', 'roomb'],\n",
    "    ball: ['ball1', 'ball2', 'ball3', 'ball4', 'ball5', 'ball6'],\n",
    "    gripper: ['left', 'right']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ground(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grounded_actions = {}\n",
    "for action_schema in model.action_schemas:\n",
    "    obj_lists_per_params = {params_type:[] for params_type in action_schema.params_types}\n",
    "    for params_type in action_schema.params_types:\n",
    "        for obj_type in objects.keys():\n",
    "            if obj_type.is_child(params_type):\n",
    "                obj_lists_per_params[params_type].extend(objects[obj_type])  \n",
    "    for obj_list in itertools.product(*[itertools.permutations(obj_lists_per_params[params_type], action_schema.params[params_type])\\\n",
    "                                          for params_type in action_schema.params_types]):\n",
    "        objects_per_action = {}\n",
    "        constructed = action_schema.name + ' ' + ' '.join([f'{action_schema.params_types[i].name} '\n",
    "                                                           +f' {action_schema.params_types[i].name} '.join(obj_list[i])\n",
    "                                                           for i in range(len(obj_list))])\n",
    "        all_grounded_actions[constructed] = len(all_grounded_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'move room rooma room roomb': 0,\n",
       " 'move room roomb room rooma': 1,\n",
       " 'pick ball ball1 gripper left room rooma': 2,\n",
       " 'pick ball ball1 gripper left room roomb': 3,\n",
       " 'pick ball ball1 gripper right room rooma': 4,\n",
       " 'pick ball ball1 gripper right room roomb': 5,\n",
       " 'pick ball ball2 gripper left room rooma': 6,\n",
       " 'pick ball ball2 gripper left room roomb': 7,\n",
       " 'pick ball ball2 gripper right room rooma': 8,\n",
       " 'pick ball ball2 gripper right room roomb': 9,\n",
       " 'pick ball ball3 gripper left room rooma': 10,\n",
       " 'pick ball ball3 gripper left room roomb': 11,\n",
       " 'pick ball ball3 gripper right room rooma': 12,\n",
       " 'pick ball ball3 gripper right room roomb': 13,\n",
       " 'pick ball ball4 gripper left room rooma': 14,\n",
       " 'pick ball ball4 gripper left room roomb': 15,\n",
       " 'pick ball ball4 gripper right room rooma': 16,\n",
       " 'pick ball ball4 gripper right room roomb': 17,\n",
       " 'pick ball ball5 gripper left room rooma': 18,\n",
       " 'pick ball ball5 gripper left room roomb': 19,\n",
       " 'pick ball ball5 gripper right room rooma': 20,\n",
       " 'pick ball ball5 gripper right room roomb': 21,\n",
       " 'pick ball ball6 gripper left room rooma': 22,\n",
       " 'pick ball ball6 gripper left room roomb': 23,\n",
       " 'pick ball ball6 gripper right room rooma': 24,\n",
       " 'pick ball ball6 gripper right room roomb': 25,\n",
       " 'drop ball ball1 gripper left room rooma': 26,\n",
       " 'drop ball ball1 gripper left room roomb': 27,\n",
       " 'drop ball ball1 gripper right room rooma': 28,\n",
       " 'drop ball ball1 gripper right room roomb': 29,\n",
       " 'drop ball ball2 gripper left room rooma': 30,\n",
       " 'drop ball ball2 gripper left room roomb': 31,\n",
       " 'drop ball ball2 gripper right room rooma': 32,\n",
       " 'drop ball ball2 gripper right room roomb': 33,\n",
       " 'drop ball ball3 gripper left room rooma': 34,\n",
       " 'drop ball ball3 gripper left room roomb': 35,\n",
       " 'drop ball ball3 gripper right room rooma': 36,\n",
       " 'drop ball ball3 gripper right room roomb': 37,\n",
       " 'drop ball ball4 gripper left room rooma': 38,\n",
       " 'drop ball ball4 gripper left room roomb': 39,\n",
       " 'drop ball ball4 gripper right room rooma': 40,\n",
       " 'drop ball ball4 gripper right room roomb': 41,\n",
       " 'drop ball ball5 gripper left room rooma': 42,\n",
       " 'drop ball ball5 gripper left room roomb': 43,\n",
       " 'drop ball ball5 gripper right room rooma': 44,\n",
       " 'drop ball ball5 gripper right room roomb': 45,\n",
       " 'drop ball ball6 gripper left room rooma': 46,\n",
       " 'drop ball ball6 gripper left room roomb': 47,\n",
       " 'drop ball ball6 gripper right room rooma': 48,\n",
       " 'drop ball ball6 gripper right room roomb': 49}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_grounded_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [00:00<00:00, 157.10it/s]\n"
     ]
    }
   ],
   "source": [
    "steps_state1 = []\n",
    "steps_action = []\n",
    "steps_state2 = []\n",
    "\n",
    "traces = generate.pddl.VanillaSampling(dom='./gripper/domain.pddl', prob='./gripper/prob01.pddl', plan_len = 10, num_traces = 10).traces\n",
    "for trace in traces:\n",
    "    # last step no action\n",
    "    for t in range(len(trace.steps)-1):\n",
    "        fluents_in_state1 = {f._serialize()[1:-1] for f in trace.steps[t].state if trace.steps[t].state[f] is True}\n",
    "        fluents_in_state2 = {f._serialize()[1:-1] for f in trace.steps[t+1].state if trace.steps[t+1].state[f] is True}\n",
    "        state1 = [1 if p in fluents_in_state1 else 0 for p in model.propositions]\n",
    "        state2 = [1 if p in fluents_in_state2 else 0 for p in model.propositions]\n",
    "        \n",
    "        # action parameter order may not be the same as our model\n",
    "        action = trace.steps[t].action\n",
    "        action_obj_params = sorted([o for o in action.obj_params], key=lambda o:o.obj_type)\n",
    "        \n",
    "        steps_action.append(all_grounded_actions[f\"{action.name} {' '.join([o.details()for o in action_obj_params])}\"])\n",
    "        \n",
    "        steps_state1.append(state1)\n",
    "        steps_state2.append(state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_state1_tensor = torch.tensor(np.array(steps_state1)).float()\n",
    "steps_action_tensor = torch.tensor(np.array(steps_action))\n",
    "steps_state2_tensor = torch.tensor(np.array(steps_state2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1000\n",
    "dataset = TensorDataset(steps_state1_tensor, steps_action_tensor, steps_state2_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_sz, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "for schema in model.action_schemas:\n",
    "    parameters.append({'params': schema.parameters(), 'lr': 1e-3})\n",
    "optimizer = optim.Adam(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 RESULTS: Average loss: 0.6226490479\n",
      "Epoch 10 RESULTS: Average loss: 0.5924045410\n",
      "Epoch 20 RESULTS: Average loss: 0.5525383301\n",
      "Epoch 30 RESULTS: Average loss: 0.5116485291\n",
      "Epoch 40 RESULTS: Average loss: 0.4767666321\n",
      "Epoch 50 RESULTS: Average loss: 0.4681913452\n",
      "Epoch 60 RESULTS: Average loss: 0.4654275513\n",
      "Epoch 70 RESULTS: Average loss: 0.4632035828\n",
      "Epoch 80 RESULTS: Average loss: 0.4623683167\n",
      "Epoch 90 RESULTS: Average loss: 0.4620728760\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "  loss_final = 0.0\n",
    "  for i, (state_1, executed_actions, state_2) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    precon, addeff, deleff = model.build(executed_actions)\n",
    "    # The result of applying a in s is (s\\Del(a)) U Add(a)\n",
    "    # We can simplfy this to be:\n",
    "    # ((p in state 1) ^ (not p in Del(a))) v ((not p in state 1) ^ (p in Add(a)))\n",
    "    # Note we implicitly apply the constraint that add effects and preconditions\n",
    "    # cannot intersect and only preconditions can be deleted\n",
    "    # The \"or\" can be translated to an addition as the two sides and exclusive\n",
    "#     preds = addeff + (1-addeff)*state_1*(1-deleff)\n",
    "    preds = state_1*(1-deleff) + (1-state_1)*addeff\n",
    "    \n",
    "    # Since we view the state_2 as true targets, we can binary cross-entropy loss\n",
    "    # If state_2 is also predicated, use KL-divergence to ensure two distributions are close?\n",
    "    loss = F.mse_loss(preds, state_2, reduction='sum')\n",
    "    # Add in validity constraint\n",
    "    # Since executed actions are applicable in state_1\n",
    "    # p in Pre(a) -> p in state_1 for all a in executed_actions\n",
    "    # not ((p in Pre(a)) ^ (not p in state_1))\n",
    "    validity_constraint = (1-state_1) * (precon)\n",
    "    loss += F.mse_loss(validity_constraint, torch.zeros(validity_constraint.shape, dtype=validity_constraint.dtype), reduction='sum')\n",
    "#     loss += model.constraint_loss()\n",
    "    loss += 0.2*F.mse_loss(precon, torch.ones(precon.shape, dtype=precon.dtype), reduction='sum')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_final += loss.item() / batch_sz\n",
    "  if epoch%10 == 0:\n",
    "    print('Epoch {} RESULTS: Average loss: {:.10f}'.format(epoch, loss_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move room a room b\n",
      "at-robby room a\n",
      "at-robby room b\n",
      "at-robby room a\n",
      "\n",
      "pick ball a gripper b room c\n",
      "at-robby room c, at ball a room c, free gripper b\n",
      "carry ball a gripper b\n",
      "at ball a room c, free gripper b\n",
      "\n",
      "drop ball a gripper b room c\n",
      "at-robby room c, carry ball a gripper b\n",
      "at ball a room c, free gripper b\n",
      "carry ball a gripper b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for action_schema in model.action_schemas:\n",
    "    action_schema.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Type(\"object\", None)\n",
    "movable = Type(\"movable\", base)\n",
    "location = Type(\"location\", base)\n",
    "city = Type(\"city\", base)\n",
    "obj = Type(\"obj\", movable)\n",
    "transport = Type(\"transport\", movable)\n",
    "truck = Type(\"truck\", transport)\n",
    "airplane = Type(\"airplane\", transport)\n",
    "airport = Type(\"airport\", location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Domain_Model([\n",
    "                        Predicate('at', {movable:1, location: 1}),\n",
    "                        Predicate('in', {obj:1, transport:1}),\n",
    "                        Predicate('in-city', {location:1, city:1}),],\n",
    "                     [\n",
    "                        Action_Schema('load-truck', {obj:1, truck:1, location:1}),\n",
    "                        Action_Schema('load-airplane', {obj:1, airplane:1, airport:1}),\n",
    "                        Action_Schema('unload-truck', {obj:1, truck:1, location:1}),\n",
    "                        Action_Schema('unload-airplane', {obj:1, airplane:1, airport:1}),\n",
    "                        Action_Schema('drive-truck', {truck:1, location:2, city:1}),\n",
    "                        Action_Schema('fly-airplane', {airplane:1, airport:2})\n",
    "                     ], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = {\n",
    "    location: [\n",
    "        'city1-1', 'city2-1'\n",
    "    ],\n",
    "    city: [\n",
    "        'city1', 'city2'\n",
    "    ],\n",
    "    obj: ['package1', 'package2', 'package3', 'package4', 'package5', 'package6',],\n",
    "    truck: ['truck1', 'truck2'],\n",
    "    airplane: ['plane1', 'plane2'],\n",
    "    airport: ['city1-2', 'city2-2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ground(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grounded_actions = {}\n",
    "for action_schema in model.action_schemas:\n",
    "    obj_lists_per_params = {params_type:[] for params_type in action_schema.params_types}\n",
    "    for params_type in action_schema.params_types:\n",
    "        for obj_type in objects.keys():\n",
    "            if obj_type.is_child(params_type):\n",
    "                obj_lists_per_params[params_type].extend(objects[obj_type])  \n",
    "    for obj_list in itertools.product(*[itertools.permutations(obj_lists_per_params[params_type], action_schema.params[params_type])\\\n",
    "                                          for params_type in action_schema.params_types]):\n",
    "        objects_per_action = {}\n",
    "        constructed = action_schema.name + ' ' + ' '.join([f'{action_schema.params_types[i].name} '\n",
    "                                                           +f' {action_schema.params_types[i].name} '.join(obj_list[i])\n",
    "                                                           for i in range(len(obj_list))])\n",
    "        all_grounded_actions[constructed] = len(all_grounded_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6Ox1xFBSKYx"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 64.44it/s]\n"
     ]
    }
   ],
   "source": [
    "steps_state1 = []\n",
    "steps_action = []\n",
    "steps_state2 = []\n",
    "\n",
    "traces = generate.pddl.VanillaSampling(dom='./logistics/domain.pddl', prob='./logistics/prob02.pddl', plan_len=10, num_traces=10).traces\n",
    "for trace in traces:\n",
    "    # last step no action\n",
    "    for t in range(len(trace.steps)-1):\n",
    "        fluents_in_state1 = set()\n",
    "        for f in trace.steps[t].state:\n",
    "            if trace.steps[t].state[f] is True:\n",
    "                if f.name=='at':\n",
    "                    serialized_list = f._serialize()[1:-1].split(' ')\n",
    "                    fluents_in_state1.add(f'at location {serialized_list[4]} movable {serialized_list[2]}')\n",
    "                elif f.name=='in':\n",
    "                    serialized_list = f._serialize()[1:-1].split(' ')\n",
    "                    fluents_in_state1.add(f'in obj {serialized_list[2]} transport {serialized_list[4]}')\n",
    "                elif f.name=='in-city':\n",
    "                    serialized_list = f._serialize()[1:-1].split(' ')\n",
    "                    fluents_in_state1.add(f'in-city city {serialized_list[4]} location {serialized_list[2]}')\n",
    "        fluents_in_state2 = set()\n",
    "        for f in trace.steps[t+1].state:\n",
    "            if trace.steps[t+1].state[f] is True:\n",
    "                if f.name=='at':\n",
    "                    serialized_list = f._serialize()[1:-1].split(' ')\n",
    "                    fluents_in_state2.add(f'at location {serialized_list[4]} movable {serialized_list[2]}')\n",
    "                elif f.name=='in':\n",
    "                    serialized_list = f._serialize()[1:-1].split(' ')\n",
    "                    fluents_in_state2.add(f'in obj {serialized_list[2]} transport {serialized_list[4]}')\n",
    "                elif f.name=='in-city':\n",
    "                    serialized_list = f._serialize()[1:-1].split(' ')\n",
    "                    fluents_in_state2.add(f'in-city city {serialized_list[4]} location {serialized_list[2]}')\n",
    "        state1 = [1 if p in fluents_in_state1 else 0 for p in model.propositions]\n",
    "        state2 = [1 if p in fluents_in_state2 else 0 for p in model.propositions]\n",
    "        \n",
    "        # action parameter order may not be the same as our model\n",
    "        action = trace.steps[t].action\n",
    "        action_obj_params = []\n",
    "        for o in action.obj_params:\n",
    "            if 'airplane' not in action.name and o.obj_type=='airport':\n",
    "                o.obj_type = 'location'\n",
    "            action_obj_params.append(o)\n",
    "        action_obj_params = sorted(action_obj_params, key=lambda o:o.obj_type)\n",
    "        \n",
    "        steps_action.append(all_grounded_actions[f\"{action.name} {' '.join([o.details()for o in action_obj_params])}\"])\n",
    "        \n",
    "        steps_state1.append(state1)\n",
    "        steps_state2.append(state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_state1_tensor = torch.tensor(np.array(steps_state1)).float()\n",
    "steps_state1_tensor[:, [64, 66, 69, 71]] = 1.0\n",
    "steps_action_tensor = torch.tensor(np.array(steps_action))\n",
    "steps_state2_tensor = torch.tensor(np.array(steps_state2)).float()\n",
    "steps_state2_tensor[:, [64, 66, 69, 71]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1000\n",
    "dataset = TensorDataset(steps_state1_tensor, steps_action_tensor, steps_state2_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_sz, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "for schema in model.action_schemas:\n",
    "    parameters.append({'params': schema.parameters(), 'lr': 1e-3})\n",
    "optimizer = optim.Adam(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 RESULTS: Average loss: 1.3849838867\n",
      "Epoch 10 RESULTS: Average loss: 1.3639166260\n",
      "Epoch 20 RESULTS: Average loss: 1.3282530518\n",
      "Epoch 30 RESULTS: Average loss: 1.2905253906\n",
      "Epoch 40 RESULTS: Average loss: 1.2696861572\n",
      "Epoch 50 RESULTS: Average loss: 1.2635958252\n",
      "Epoch 60 RESULTS: Average loss: 1.2611717529\n",
      "Epoch 70 RESULTS: Average loss: 1.2597830811\n",
      "Epoch 80 RESULTS: Average loss: 1.2593234863\n",
      "Epoch 90 RESULTS: Average loss: 1.2591214600\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "  optimizer.zero_grad()\n",
    "  loss_final = 0.0\n",
    "  for i, (state_1, executed_actions, state_2) in enumerate(dataloader):\n",
    "    precon, addeff, deleff = model.build(executed_actions)\n",
    "    # The result of applying a in s is (s\\Del(a)) U Add(a)\n",
    "    # We can simplfy this to be:\n",
    "    # ((p in state 1) ^ (not p in Del(a))) v ((not p in state 1) ^ (p in Add(a)))\n",
    "    # Note we implicitly apply the constraint that add effects and preconditions\n",
    "    # cannot intersect and only preconditions can be deleted\n",
    "    # The \"or\" can be translated to an addition as the two sides and exclusive\n",
    "#     preds = addeff + (1-addeff)*state_1*(1-deleff)\n",
    "    preds = 1- (1-state_1*(1-deleff)) * (1-(1-state_1)*addeff)\n",
    "    \n",
    "    # Since we view the state_2 as true targets, we can binary cross-entropy loss\n",
    "    # If state_2 is also predicated, use KL-divergence to ensure two distributions are close?\n",
    "    loss = F.mse_loss(preds, state_2, reduction='sum')\n",
    "    # Add in validity constraint\n",
    "    # Since executed actions are applicable in state_1\n",
    "    # p in Pre(a) -> p in state_1 for all a in executed_actions\n",
    "    # not ((p in Pre(a)) ^ (not p in state_1))\n",
    "    validity_constraint = (1-state_1) * (precon)\n",
    "    loss += F.mse_loss(validity_constraint, torch.zeros(validity_constraint.shape, dtype=validity_constraint.dtype), reduction='sum')\n",
    "#     loss += model.constraint_loss()\n",
    "    loss += 0.2*F.mse_loss(precon, torch.ones(precon.shape, dtype=precon.dtype), reduction='sum')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_final += loss.item() / batch_sz\n",
    "  if epoch%10 == 0:\n",
    "    print('Epoch {} RESULTS: Average loss: {:.10f}'.format(epoch, loss_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load-truck location a obj b truck c\n",
      "at location a movable b, at location a movable c\n",
      "in obj b transport c\n",
      "at location a movable b\n",
      "\n",
      "load-airplane airplane a airport b obj c\n",
      "at location b movable a, at location b movable c\n",
      "in obj c transport a\n",
      "at location b movable c\n",
      "\n",
      "unload-truck location a obj b truck c\n",
      "at location a movable c, in obj b transport c\n",
      "at location a movable b\n",
      "in obj b transport c\n",
      "\n",
      "unload-airplane airplane a airport b obj c\n",
      "at location b movable a, in obj c transport a\n",
      "at location b movable c\n",
      "in obj c transport a\n",
      "\n",
      "drive-truck city a location b location c truck d\n",
      "at location b movable d, in-city city a location b, in-city city a location c\n",
      "at location c movable d\n",
      "at location b movable d\n",
      "\n",
      "fly-airplane airplane a airport b airport c\n",
      "at location b movable a\n",
      "at location c movable a\n",
      "at location b movable a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for action_schema in model.action_schemas:\n",
    "    action_schema.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
